{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.functions import avg, max, min, count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Test1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-HU9P1HB:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2587da585f0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = spark.read.csv('archive/features.csv', header=True,inferSchema=True)\n",
    "s1 = spark.read.csv('archive/stores.csv', header=True,inferSchema=True)\n",
    "te1 = spark.read.csv('archive/test.csv', header=True,inferSchema=True)\n",
    "tr1 = spark.read.csv('archive/train.csv', header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+---------+\n",
      "|Store|      Date|Temperature|Fuel_Price|MarkDown1|MarkDown2|MarkDown3|MarkDown4|MarkDown5|        CPI|Unemployment|IsHoliday|\n",
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+---------+\n",
      "|    1|2010-02-05|      42.31|     2.572|       NA|       NA|       NA|       NA|       NA|211.0963582|       8.106|    false|\n",
      "|    1|2010-02-12|      38.51|     2.548|       NA|       NA|       NA|       NA|       NA|211.2421698|       8.106|     true|\n",
      "|    1|2010-02-19|      39.93|     2.514|       NA|       NA|       NA|       NA|       NA|211.2891429|       8.106|    false|\n",
      "|    1|2010-02-26|      46.63|     2.561|       NA|       NA|       NA|       NA|       NA|211.3196429|       8.106|    false|\n",
      "|    1|2010-03-05|       46.5|     2.625|       NA|       NA|       NA|       NA|       NA|211.3501429|       8.106|    false|\n",
      "|    1|2010-03-12|      57.79|     2.667|       NA|       NA|       NA|       NA|       NA|211.3806429|       8.106|    false|\n",
      "|    1|2010-03-19|      54.58|      2.72|       NA|       NA|       NA|       NA|       NA| 211.215635|       8.106|    false|\n",
      "|    1|2010-03-26|      51.45|     2.732|       NA|       NA|       NA|       NA|       NA|211.0180424|       8.106|    false|\n",
      "|    1|2010-04-02|      62.27|     2.719|       NA|       NA|       NA|       NA|       NA|210.8204499|       7.808|    false|\n",
      "|    1|2010-04-09|      65.86|      2.77|       NA|       NA|       NA|       NA|       NA|210.6228574|       7.808|    false|\n",
      "|    1|2010-04-16|      66.32|     2.808|       NA|       NA|       NA|       NA|       NA|   210.4887|       7.808|    false|\n",
      "|    1|2010-04-23|      64.84|     2.795|       NA|       NA|       NA|       NA|       NA|210.4391228|       7.808|    false|\n",
      "|    1|2010-04-30|      67.41|      2.78|       NA|       NA|       NA|       NA|       NA|210.3895456|       7.808|    false|\n",
      "|    1|2010-05-07|      72.55|     2.835|       NA|       NA|       NA|       NA|       NA|210.3399684|       7.808|    false|\n",
      "|    1|2010-05-14|      74.78|     2.854|       NA|       NA|       NA|       NA|       NA|210.3374261|       7.808|    false|\n",
      "|    1|2010-05-21|      76.44|     2.826|       NA|       NA|       NA|       NA|       NA|210.6170934|       7.808|    false|\n",
      "|    1|2010-05-28|      80.44|     2.759|       NA|       NA|       NA|       NA|       NA|210.8967606|       7.808|    false|\n",
      "|    1|2010-06-04|      80.69|     2.705|       NA|       NA|       NA|       NA|       NA|211.1764278|       7.808|    false|\n",
      "|    1|2010-06-11|      80.43|     2.668|       NA|       NA|       NA|       NA|       NA|211.4560951|       7.808|    false|\n",
      "|    1|2010-06-18|      84.11|     2.637|       NA|       NA|       NA|       NA|       NA|211.4537719|       7.808|    false|\n",
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are type casting the correct datatypes since even after infer_schema =True data types were considered wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Temperature: float (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- MarkDown1: float (nullable = true)\n",
      " |-- MarkDown2: float (nullable = true)\n",
      " |-- MarkDown3: float (nullable = true)\n",
      " |-- MarkDown4: float (nullable = true)\n",
      " |-- MarkDown5: float (nullable = true)\n",
      " |-- CPI: float (nullable = true)\n",
      " |-- Unemployment: float (nullable = true)\n",
      " |-- IsHoliday: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Typecasting data type since even after infer_schema =True data types were considered wrong\n",
    "\n",
    "f1 = f1.withColumn(\"Store\",col(\"Store\").cast(\"Integer\"))\n",
    "f1 = f1.withColumn(\"Date\",col(\"Date\").cast(\"Date\"))\n",
    "f1 = f1.withColumn(\"Temperature\",col(\"Temperature\").cast(\"Float\"))\n",
    "\n",
    "f1 = f1.withColumn(\"MarkDown1\",col(\"MarkDown1\").cast(\"Float\"))\n",
    "f1 = f1.withColumn(\"MarkDown2\",col(\"MarkDown2\").cast(\"Float\"))\n",
    "f1 = f1.withColumn(\"MarkDown3\",col(\"MarkDown3\").cast(\"Float\"))\n",
    "f1 = f1.withColumn(\"MarkDown4\",col(\"MarkDown4\").cast(\"Float\"))\n",
    "f1 = f1.withColumn(\"MarkDown5\",col(\"MarkDown5\").cast(\"Float\"))\n",
    "f1 = f1.withColumn(\"CPI\",col(\"CPI\").cast(\"Float\"))\n",
    "f1 = f1.withColumn(\"Unemployment\",col(\"Unemployment\").cast(\"Float\"))\n",
    "f1 = f1.withColumn(\"IsHoliday\",col(\"IsHoliday\").cast(\"Boolean\"))\n",
    "f1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Temperature: float (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- MarkDown1: float (nullable = true)\n",
      " |-- MarkDown2: float (nullable = true)\n",
      " |-- MarkDown3: float (nullable = true)\n",
      " |-- MarkDown4: float (nullable = true)\n",
      " |-- MarkDown5: float (nullable = true)\n",
      " |-- CPI: float (nullable = true)\n",
      " |-- Unemployment: float (nullable = true)\n",
      " |-- IsHoliday: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|MarkDown1|\n",
      "+---------+\n",
      "|  6277.39|\n",
      "| 10165.22|\n",
      "|   2124.1|\n",
      "|  4018.39|\n",
      "|  2301.44|\n",
      "| 46932.68|\n",
      "|   516.47|\n",
      "|  8081.05|\n",
      "|  5659.29|\n",
      "|  7866.57|\n",
      "| 11685.51|\n",
      "|  5000.58|\n",
      "|  5857.72|\n",
      "|  7011.68|\n",
      "|   717.12|\n",
      "| 28078.98|\n",
      "| 21068.05|\n",
      "|  7505.03|\n",
      "| 11611.99|\n",
      "|  8692.55|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Handling Missing Data: Fill or remove missing values in MarkDown1-5 (often missing in real-world sales data).\n",
    "f1.select('MarkDown1').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find out the Number of null values in each column of Markdown1 - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4158 5269 4577 4726 4140\n"
     ]
    }
   ],
   "source": [
    "# finding number of Null values in MarkDown1-5\n",
    "print(f1.where((f1.MarkDown1.isNull())).count(),\n",
    "f1.where((f1.MarkDown2.isNull())).count(),\n",
    "f1.where((f1.MarkDown3.isNull())).count(),\n",
    "f1.where((f1.MarkDown4.isNull())).count(),\n",
    "f1.where((f1.MarkDown5.isNull())).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7032.371786093377\n",
      "3384.176592808865\n",
      "1760.100175767131\n",
      "3292.9358917338986\n"
     ]
    }
   ],
   "source": [
    "# Finding Mean values in each column\n",
    "m1_mean = f1.select(mean(\"MarkDown1\")).collect()[0][0]\n",
    "\n",
    "m2_mean = f1.select(mean(\"MarkDown2\")).collect()[0][0]\n",
    "m3_mean = f1.select(mean(\"MarkDown3\")).collect()[0][0]\n",
    "m4_mean = f1.select(mean(\"MarkDown4\")).collect()[0][0]\n",
    "m5_mean = f1.select(mean(\"MarkDown5\")).collect()[0][0]\n",
    "print(m1_mean)\n",
    "print(m2_mean)\n",
    "print(m3_mean)\n",
    "print(m4_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=f1.fillna({\"MarkDown1\":m1_mean})\n",
    "f1=f1.fillna({\"MarkDown2\":m2_mean})\n",
    "f1=f1.fillna({\"MarkDown3\":m3_mean})\n",
    "f1=f1.fillna({\"MarkDown4\":m4_mean})\n",
    "f1=f1.fillna({\"MarkDown5\":m5_mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "# Checking number of Null values after process in MarkDown1-5\n",
    "print(f1.where((f1.MarkDown1.isNull())).count(),\n",
    "f1.where((f1.MarkDown2.isNull())).count(),\n",
    "f1.where((f1.MarkDown3.isNull())).count(),\n",
    "f1.where((f1.MarkDown4.isNull())).count(),\n",
    "f1.where((f1.MarkDown5.isNull())).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have identified null values in Markdown1 - 5 columns and replaced it with mean value of specific columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Lets check if Date column is in the \"yyyy-MM-dd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Date|\n",
      "+----------+\n",
      "|2010-02-05|\n",
      "|2010-02-12|\n",
      "|2010-02-19|\n",
      "|2010-02-26|\n",
      "|2010-03-05|\n",
      "|2010-03-12|\n",
      "|2010-03-19|\n",
      "|2010-03-26|\n",
      "|2010-04-02|\n",
      "|2010-04-09|\n",
      "|2010-04-16|\n",
      "|2010-04-23|\n",
      "|2010-04-30|\n",
      "|2010-05-07|\n",
      "|2010-05-14|\n",
      "|2010-05-21|\n",
      "|2010-05-28|\n",
      "|2010-06-04|\n",
      "|2010-06-11|\n",
      "|2010-06-18|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1.select(\"Date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence no format changing needed for date column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate a 4-Week Moving Average for the train.csv file to make forecasting easy in Dashboard graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr1 = tr1.withColumn(\"Weekly_Sales\",col(\"Weekly_Sales\").cast(\"Float\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- Dept: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Weekly_Sales: float (nullable = true)\n",
      " |-- IsHoliday: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Window for 4-Week Moving Average (Including Current Row)\n",
    "window_spec = Window.partitionBy(\"Store\", \"Dept\").orderBy(\"Date\").rowsBetween(-3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Weekly Moving Average\n",
    "tr1 = tr1.withColumn(\"4_Week_Moving_Avg\", avg(col(\"Weekly_Sales\")).over(window_spec))\n",
    "tr1 = tr1.withColumn(\"4_Week_Moving_Avg\",col(\"4_Week_Moving_Avg\").cast(\"Float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------------+---------+-----------------+\n",
      "|Store|Dept|      Date|Weekly_Sales|IsHoliday|4_Week_Moving_Avg|\n",
      "+-----+----+----------+------------+---------+-----------------+\n",
      "|    1|   2|2010-02-05|    50605.27|    false|         50605.27|\n",
      "|    1|   2|2010-02-12|    44682.74|     true|        47644.004|\n",
      "|    1|   2|2010-02-19|    47928.89|    false|        47738.965|\n",
      "|    1|   2|2010-02-26|    44292.87|    false|         46877.44|\n",
      "|    1|   2|2010-03-05|    48397.98|    false|         46325.62|\n",
      "|    1|   2|2010-03-12|    43751.94|    false|         46092.92|\n",
      "|    1|   2|2010-03-19|    43615.49|    false|         45014.57|\n",
      "|    1|   2|2010-03-26|    41892.55|    false|        44414.492|\n",
      "|    1|   2|2010-04-02|     47450.5|    false|         44177.62|\n",
      "|    1|   2|2010-04-09|    46549.73|    false|        44877.066|\n",
      "|    1|   2|2010-04-16|    45025.02|    false|         45229.45|\n",
      "|    1|   2|2010-04-23|    44418.11|    false|         45860.84|\n",
      "|    1|   2|2010-04-30|     45971.3|    false|         45491.04|\n",
      "|    1|   2|2010-05-07|    47903.01|    false|         45829.36|\n",
      "|    1|   2|2010-05-14|    43675.61|    false|        45492.008|\n",
      "|    1|   2|2010-05-21|    44319.15|    false|        45467.266|\n",
      "|    1|   2|2010-05-28|    44619.52|    false|         45129.32|\n",
      "|    1|   2|2010-06-04|    48754.47|    false|        45342.188|\n",
      "|    1|   2|2010-06-11|    47089.54|    false|        46195.668|\n",
      "|    1|   2|2010-06-18|    44428.71|    false|         46223.06|\n",
      "+-----+----+----------+------------+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|min(Weekly_Sales)|\n",
      "+-----------------+\n",
      "|         -4988.94|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr1.select(min(\"Weekly_Sales\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating Total_Weekly_Sales, Avg_Weekly_Sales, Max_Weekly_Sales, Min_Weekly_Sales, Total_Transactions per Store to rank top performers as storewise_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+----------------+----------------+----------------+------------------+\n",
      "|Store|Total_Weekly_Sales|Avg_Weekly_Sales|Max_Weekly_Sales|Min_Weekly_Sales|Total_Transactions|\n",
      "+-----+------------------+----------------+----------------+----------------+------------------+\n",
      "|    1|      222402808.88|        21710.54|       203670.47|         -863.00|             10244|\n",
      "|    2|      275382440.86|        26898.07|       285353.53|        -1098.00|             10238|\n",
      "|    3|       57586735.05|         6373.03|       155897.94|        -1008.96|              9036|\n",
      "|    4|      299543953.46|        29161.21|       385051.03|         -898.00|             10272|\n",
      "|    5|       45475688.87|         5053.42|        93517.72|         -101.26|              8999|\n",
      "|    6|      223756130.79|        21913.24|       342578.66|         -698.00|             10211|\n",
      "|    7|       81598275.18|         8358.77|       222921.09|         -459.00|              9762|\n",
      "|    8|      129951181.15|        13133.01|       153431.69|         -100.00|              9895|\n",
      "|    9|       77789219.06|         8772.89|       139427.44|         -496.00|              8867|\n",
      "|   10|      271617713.76|        26332.30|       693099.38|         -798.00|             10315|\n",
      "|   11|      193962786.80|        19276.76|       245767.47|         -594.00|             10062|\n",
      "|   12|      144287230.04|        14867.31|       360140.66|         -598.00|              9705|\n",
      "|   13|      286517703.72|        27355.14|       292165.78|          -98.00|             10474|\n",
      "|   14|      288999911.26|        28784.85|       474330.09|         -498.00|             10040|\n",
      "|   15|       89133683.95|         9002.49|       292555.25|         -179.00|              9901|\n",
      "|   16|       74252425.44|         7863.22|       129372.97|        -1699.00|              9443|\n",
      "|   17|      127782138.72|        12954.39|       194660.52|         -259.00|              9864|\n",
      "|   18|      155114734.11|        15733.31|       353008.63|         -259.00|              9859|\n",
      "|   19|      206634861.96|        20362.13|       339846.03|         -649.00|             10148|\n",
      "|   20|      301397792.45|        29508.30|       422306.25|         -798.00|             10214|\n",
      "+-----+------------------+----------------+----------------+----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#grouping Weekly_Sales per Store\n",
    "\n",
    "storewise_stats = (\n",
    "    tr1.groupBy(\"Store\")\n",
    "    .agg(\n",
    "        sum(\"Weekly_Sales\").alias(\"Total_Weekly_Sales\"),\n",
    "        avg(\"Weekly_Sales\").alias(\"Avg_Weekly_Sales\"),\n",
    "        max(\"Weekly_Sales\").alias(\"Max_Weekly_Sales\"),\n",
    "        min(\"Weekly_Sales\").alias(\"Min_Weekly_Sales\"),\n",
    "        count(\"Weekly_Sales\").alias(\"Total_Transactions\")  # Counts number of sales records per store\n",
    "    )\n",
    "    .orderBy(\"Store\")\n",
    "    .withColumn(\"Total_Weekly_Sales\", col(\"Total_Weekly_Sales\").cast(\"decimal(20,2)\"))\n",
    "    .withColumn(\"Avg_Weekly_Sales\", col(\"Avg_Weekly_Sales\").cast(\"decimal(20,2)\"))\n",
    "    .withColumn(\"Max_Weekly_Sales\", col(\"Max_Weekly_Sales\").cast(\"decimal(20,2)\"))\n",
    "    .withColumn(\"Min_Weekly_Sales\", col(\"Min_Weekly_Sales\").cast(\"decimal(20,2)\"))\n",
    ")\n",
    "\n",
    "storewise_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are joining Stores.csv to storewise_stats for creating one more df for data analytics and BI reports called Transformedfile1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+----------------+----------------+----------------+------------------+----+------+\n",
      "|Store|Total_Weekly_Sales|Avg_Weekly_Sales|Max_Weekly_Sales|Min_Weekly_Sales|Total_Transactions|Type|  Size|\n",
      "+-----+------------------+----------------+----------------+----------------+------------------+----+------+\n",
      "|   12|      144287230.04|        14867.31|       360140.66|         -598.00|              9705|   B|112238|\n",
      "|    1|      222402808.88|        21710.54|       203670.47|         -863.00|             10244|   A|151315|\n",
      "|   13|      286517703.72|        27355.14|       292165.78|          -98.00|             10474|   A|219622|\n",
      "|    6|      223756130.79|        21913.24|       342578.66|         -698.00|             10211|   A|202505|\n",
      "|    3|       57586735.05|         6373.03|       155897.94|        -1008.96|              9036|   B| 37392|\n",
      "|    5|       45475688.87|         5053.42|        93517.72|         -101.26|              8999|   B| 34875|\n",
      "|   15|       89133683.95|         9002.49|       292555.25|         -179.00|              9901|   B|123737|\n",
      "|    9|       77789219.06|         8772.89|       139427.44|         -496.00|              8867|   B|125833|\n",
      "|    4|      299543953.46|        29161.21|       385051.03|         -898.00|             10272|   A|205863|\n",
      "|    8|      129951181.15|        13133.01|       153431.69|         -100.00|              9895|   A|155078|\n",
      "|    7|       81598275.18|         8358.77|       222921.09|         -459.00|              9762|   B| 70713|\n",
      "|   10|      271617713.76|        26332.30|       693099.38|         -798.00|             10315|   B|126512|\n",
      "|   11|      193962786.80|        19276.76|       245767.47|         -594.00|             10062|   A|207499|\n",
      "|   14|      288999911.26|        28784.85|       474330.09|         -498.00|             10040|   A|200898|\n",
      "|    2|      275382440.86|        26898.07|       285353.53|        -1098.00|             10238|   A|202307|\n",
      "|   28|      189263680.56|        18714.89|       355356.38|        -4988.94|             10113|   A|206302|\n",
      "|   26|      143416393.74|        14554.13|       196615.88|          -79.00|              9854|   A|152513|\n",
      "|   27|      253855917.05|        24826.98|       420586.56|         -409.00|             10225|   A|204184|\n",
      "|   22|      147075648.46|        15181.22|       393705.19|         -175.54|              9688|   B|119557|\n",
      "|   16|       74252425.44|         7863.22|       129372.97|        -1699.00|              9443|   B| 57197|\n",
      "+-----+------------------+----------------+----------------+----------------+------------------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Transformedfile1 = storewise_stats.join(s1, on=\"Store\", how=\"left\")\n",
    "Transformedfile1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+---------+------------+---------+----+------------+---------+-----------------+\n",
      "|Store|      Date|Temperature|Fuel_Price|MarkDown1|MarkDown2|MarkDown3|MarkDown4|MarkDown5|      CPI|Unemployment|IsHoliday|Dept|Weekly_Sales|IsHoliday|4_Week_Moving_Avg|\n",
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+---------+------------+---------+----+------------+---------+-----------------+\n",
      "|    1|2010-02-05|      42.31|     2.572|7032.3716|3384.1765|1760.1002|3292.9358|4132.2163|211.09636|       8.106|    false|   2|    50605.27|    false|         50605.27|\n",
      "|    1|2010-02-12|      38.51|     2.548|7032.3716|3384.1765|1760.1002|3292.9358|4132.2163|211.24217|       8.106|     true|   2|    44682.74|     true|        47644.004|\n",
      "|    1|2010-02-19|      39.93|     2.514|7032.3716|3384.1765|1760.1002|3292.9358|4132.2163|211.28914|       8.106|    false|   2|    47928.89|    false|        47738.965|\n",
      "|    1|2010-02-26|      46.63|     2.561|7032.3716|3384.1765|1760.1002|3292.9358|4132.2163|211.31964|       8.106|    false|   2|    44292.87|    false|         46877.44|\n",
      "|    1|2010-03-05|       46.5|     2.625|7032.3716|3384.1765|1760.1002|3292.9358|4132.2163|211.35014|       8.106|    false|   2|    48397.98|    false|         46325.62|\n",
      "|    1|2010-03-12|      57.79|     2.667|7032.3716|3384.1765|1760.1002|3292.9358|4132.2163|211.38065|       8.106|    false|   2|    43751.94|    false|         46092.92|\n",
      "|    1|2010-03-19|      54.58|      2.72|7032.3716|3384.1765|1760.1002|3292.9358|4132.2163|211.21564|       8.106|    false|   2|    43615.49|    false|         45014.57|\n",
      "|    1|2010-03-26|      51.45|     2.732|7032.3716|3384.1765|1760.1002|3292.9358|4132.2163|211.01804|       8.106|    false|   2|    41892.55|    false|        44414.492|\n",
      "|    1|2010-04-02|      62.27|     2.719|7032.3716|3384.1765|1760.1002|3292.9358|4132.2163|210.82045|       7.808|    false|   2|     47450.5|    false|         44177.62|\n",
      "|    1|2010-04-09|      65.86|      2.77|7032.3716|3384.1765|1760.1002|3292.9358|4132.2163|210.62286|       7.808|    false|   2|    46549.73|    false|        44877.066|\n",
      "|    1|2010-04-16|      66.32|     2.808|7032.3716|3384.1765|1760.1002|3292.9358|4132.2163| 210.4887|       7.808|    false|   2|    45025.02|    false|         45229.45|\n",
      "|    1|2010-04-23|      64.84|     2.795|7032.3716|3384.1765|1760.1002|3292.9358|4132.2163|210.43912|       7.808|    false|   2|    44418.11|    false|         45860.84|\n",
      "|    1|2010-04-30|      67.41|      2.78|7032.3716|3384.1765|1760.1002|3292.9358|4132.2163|210.38954|       7.808|    false|   2|     45971.3|    false|         45491.04|\n",
      "|    1|2010-05-07|      72.55|     2.835|7032.3716|3384.1765|1760.1002|3292.9358|4132.2163|210.33997|       7.808|    false|   2|    47903.01|    false|         45829.36|\n",
      "|    1|2010-05-14|      74.78|     2.854|7032.3716|3384.1765|1760.1002|3292.9358|4132.2163|210.33743|       7.808|    false|   2|    43675.61|    false|        45492.008|\n",
      "|    1|2010-05-21|      76.44|     2.826|7032.3716|3384.1765|1760.1002|3292.9358|4132.2163| 210.6171|       7.808|    false|   2|    44319.15|    false|        45467.266|\n",
      "|    1|2010-05-28|      80.44|     2.759|7032.3716|3384.1765|1760.1002|3292.9358|4132.2163|210.89676|       7.808|    false|   2|    44619.52|    false|         45129.32|\n",
      "|    1|2010-06-04|      80.69|     2.705|7032.3716|3384.1765|1760.1002|3292.9358|4132.2163|211.17642|       7.808|    false|   2|    48754.47|    false|        45342.188|\n",
      "|    1|2010-06-11|      80.43|     2.668|7032.3716|3384.1765|1760.1002|3292.9358|4132.2163| 211.4561|       7.808|    false|   2|    47089.54|    false|        46195.668|\n",
      "|    1|2010-06-18|      84.11|     2.637|7032.3716|3384.1765|1760.1002|3292.9358|4132.2163|211.45377|       7.808|    false|   2|    44428.71|    false|         46223.06|\n",
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+---------+------------+---------+----+------------+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1_tr1_join = f1.join(tr1, [\"Store\", \"Date\"], \"inner\")\n",
    "f1_tr1_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Transformedfile1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#f1_tr1_join.schema.names\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mTransformedfile1\u001b[49m\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mnames\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Transformedfile1' is not defined"
     ]
    }
   ],
   "source": [
    "#f1_tr1_join.schema.names\n",
    "Transformedfile1.schema.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the transformed data frames as csv files: \n",
    "\n",
    "f1_tr1_join as joined_data.csv, \n",
    "Transformedfile1 as store_wise_stats.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformedfile1.write.option(\"header\",\"true\").csv(\"transformed_data/store_wise_stats.csv\",header=True)\n",
    "# f1_tr1_join.write.option(\"header\",\"true\").csv(\"transformed_data/joined_data.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o956.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$3211/2115267269.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1$$Lambda$4120/1055343933.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1705/204027375.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1695/337983567.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$Lambda$1294/306501241.apply(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:497)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Write Transformedfile1 to CSV\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mTransformedfile1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransformed_data/store_wise_stats.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Write f1_tr1_join to CSV\u001b[39;00m\n\u001b[0;32m      5\u001b[0m f1_tr1_join\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformed_data/joined_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Vidit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   1863\u001b[0m )\n\u001b[1;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vidit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Vidit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Vidit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o956.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$3211/2115267269.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1$$Lambda$4120/1055343933.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1705/204027375.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1695/337983567.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$Lambda$1294/306501241.apply(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:497)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "# Write Transformedfile1 to CSV\n",
    "Transformedfile1.write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"transformed_data/store_wise_stats.csv\")\n",
    "\n",
    "# Write f1_tr1_join to CSV\n",
    "f1_tr1_join.write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"transformed_data/joined_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
